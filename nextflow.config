params {
  // Inputs
  input  = null         // samplesheet CSV
  reads  = null         // glob for quick mode
  outdir = "${projectDir}/results"

  // SPLICE Data Platform specific parameters
  job_id = null         // UUID job identifier
  short_job_id = null   // Short job identifier for SLURM job names

  // Packaging strategy: 'conda' or 'apptainer'
  packaging = null

  // Apptainer SIF locations for TRE
  fastqc_sif  = null
  multiqc_sif = null

  // Behaviour
  publish_dir_mode = 'copy'
  skip_fastqc   = false
  skip_multiqc  = false
  multiqc_config = null

  // Resource caps
  max_cpus   = 8
  max_memory = '32 GB'
  max_time   = '24h'

  // Methods
  methods_description = "${projectDir}/assets/methods_description_template.yml"
}

// Base defaults
includeConfig 'conf/base.config'

profiles {
  test {
    includeConfig 'conf/test.config'
    process {
      executor = 'local'
    }
  }
  debug {
    dumpHashes = true
    cleanup    = false
  }
  slurm {
    process {
      executor = 'slurm'
    }
  }
  singularity {
    singularity {
      enabled    = true
      autoMounts = true
    }
    docker {
      enabled = false
    }
    conda {
      enabled = false
    }
  }
  apptainer {
    params {
      packaging = 'apptainer'
    }
    apptainer {
      enabled = true
    }
    docker {
      enabled = false
    }
    conda {
      enabled = false
    }
  }
  conda {
    params {
      packaging = 'conda'
    }
    conda {
      enabled = true
      useMamba = true
      cacheDir = "${projectDir}/.conda-cache"
    }
    docker {
      enabled = false
    }
    singularity {
      enabled = false
    }
  }
  docker {
    docker {
      enabled = true
    }
    conda {
      enabled = false
    }
    singularity {
      enabled = false
    }
  }
  splice_cruksi_hpc {
    params {
      packaging = 'singulrity'
      // Fixed paths based on the data platform structure
      // Assumes workflow is run from output_data directory
      
      // DON'T set defaults here - let the workflow logic handle the priority
      // The workflow checks: if (params.input) -> samplesheet, else if (params.reads) -> glob
      // Only override if user doesn't specify either input or reads
      outdir = "./${params.job_id ?: 'unknown_job'}_output"
      publish_dir_mode = 'copy'
    }

    process {
      executor = 'slurm'
      // Add short_job_id prefix to SLURM job names
      clusterOptions = { 
        def jobName = params.short_job_id ? "--job-name=${params.short_job_id}_${task.process}" : "--job-name=${task.process}"
        def partition = "--partition=compute"
        "${jobName} ${partition}"
      }
    }

    singularity {
      enabled = true
      autoMounts = true
      // Set cache directory to the containers folder
      cacheDir = "../input_data/containers"
    }

    docker {
      enabled = false
    }

    conda {
      enabled = false
    }
  }


}

plugins {
  id 'nf-schema@2.4.2'
}
